\chapter{GPU implementation}
\label{ch:implementation}

This chapter describes how the simulation was implemented on the GPU.

The reference algorithm for evaluating
this kind of computation on the GPU
is the one by Nyland et.~al~\cite[\S31]{gems3},
which shows how to integrate a body system
subject to gravitational interactions.

\section{Work distribution}
\label{sec:work-distribution}

The simulation of the Lamb-Oseen vortex
with the algorithm described in chapter~\ref{ch:vm-design}
amounts to the numerical integration
of the convection and diffusion equations
for each of the~\(N\) particles of the system:
\begin{align}
    \label{eq:x-integration}
    x_p^{(n + 1)} &= x_p^{(n)} + u_p^{(n)}\, δt, \\
    \label{eq:y-integration}
    y_p^{(n + 1)} &= y_p^{(n)} + v_p^{(n)}\, δt, \\
    \label{eq:a-integration}
    α_p^{(n + 1)} &= α_p^{(n)} + \dot{α}_p^{(n)}\, δt.
\end{align}

At every time step~\(n\),
and for every particle~\(p\),
the values of \(u_p\), \(v_p\) and \(\dot{α}_p\)
must be evaluated as a function of all other particles
using the Biot-Savart law and the Particle Strength Exchange formula:
\begin{align}
    %\label{eq:evaluation}
%    (u_p, v_p) &= \sum_q α_p \K_ε(\x_p - \x_q) \\
%    \dot{α}_p  &= νε^{-2} \sum_q [α_q - α_p]\,η_ε(\x_p - \x_q)
    \label{eq:bs-sum}
    (u_p, v_p) &= \sum_q \vec{U}_{pq},   & \text{where } \vec{U}_{pq} &= α_p \K_ε(\x_p - \x_q), \text{and} \\
    \label{eq:pse-sum}
    \dot{α}_p  &= νε^{-2} \sum_q A_{pq}, & \text{where }       A_{pq} &= [α_q - α_p]\,η_ε(\x_p - \x_q).
\end{align}
Sums~\eqref{eq:bs-sum} and~\eqref{eq:pse-sum} can be computed
independently among particles,
provided that the descriptions~\(x_q, y_q, α_q\)
for all the particles are available.

On the GPU, thus,
each thread is assigned the job
of computing sums~\eqref{eq:bs-sum} and~\eqref{eq:pse-sum} for one particle,
and of updating the particle description
according to equations~\eqref{eq:x-integration},
\eqref{eq:y-integration} and~\eqref{eq:a-integration}.

The~\(N\) threads are grouped in \(B\)~blocks of \(T = N/B\)~threads
(for the time being, let us assume that~\(B\mid N\)).
All threads within a block execute concurrently
(but not necessarily simultaneously),
while all blocks are scheduled independent of each other
to be executed when multiprocessors are available.

Thread~\(t\) in block~\(b\) evaluates \(\vec u_p\) and \(\dot{α}_p\)
for particle~\(p = bT + t\).
Conversely,
particle~\(p\) is evaluated by thread~\(t = p\bmod T\)
in block~\(b = \lfloor p/T \rfloor\).

\tikzstyle{innergrid}=[gray!80]
\tikzstyle{tile}=[green!60!black, thick]
\tikzstyle{sync}=[red!60!black, thick, dashed]
\tikzstyle{thread}=[->, decorate, decoration={snake, amplitude=.6}]
\tikzstyle{examplecell}=[fill=orange, opacity=.5]
\tikzstyle{kernelcall}=[#1, rounded corners=2pt, very thick, pattern color=#1]
\tikzstyle{kernelarrow}=[->, thick, out=90, in=180]
\tikzstyle{imglabel}=[anchor=west, text opacity=1.0, fill=white, fill opacity=.5,
                      text height=1ex, text depth=.25ex, rounded corners]

\newcommand\drawthread[4]{%
  \pgfmathsetmacro{\ybot}{(1 + \nrblockbodies) * #2}
  \draw [#1] (0.5, \ybot + 0.5 + #3) -- ++(#4 - 1, 0);%
}

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=.2, yscale=-1, font=\small]

    \def\nrbodies{64}     \def\lastbody{63}
    \def\nrblocks{8}      \def\lastblock{7}
    \def\nrtiles{8}       \def\lasttile{7}
    \def\nrblockbodies{8} \def\lastblockbody{7}

    %\def\nrbodies{42}     \def\lastbody{41}
    %\def\nrblocks{7}      \def\lastblock{6}
    %\def\nrtiles{7}       \def\lasttile{6}
    %\def\nrblockbodies{6} \def\lastblockbody{5}

    \foreach \block in {0,1,...,\lastblock} {
      %\def\ybot{9 * \block}
      \pgfmathsetmacro{\ybot}{(1 + \nrblockbodies) * \block}
      \draw[innergrid] (0, \ybot) grid      ++(\nrbodies, \nrblockbodies);
      \draw[tile]      (0, \ybot) rectangle ++(\nrbodies, \nrblockbodies);
      \draw (1 + \nrbodies, \ybot + \nrblockbodies / 2)
          node[anchor=west] { block $\block$ };

      \foreach \syncpoint in {1,...,\lasttile}
        \draw[sync] (\nrtiles * \syncpoint, \ybot) -- ++(0, \nrblockbodies);
    }

    \def\block{2}
    \foreach \k in {0,...,3}
      \drawthread{thread}{\block}{\k}{43};
    \foreach \k in {4,...,\lastblockbody}
      \drawthread{thread}{\block}{\k}{42};

    \def\block{1}
    \foreach \k in {0,1}
      \drawthread{thread}{\block}{\k}{14};
    \foreach \k in {2,...,\lastblockbody}
      \drawthread{thread}{\block}{\k}{11};

    \def\block{3}
    \foreach \k in {0,...,\lastblockbody}
      \drawthread{thread}{\block}{\k}{24.5};

    \def\block{4}
    \def\to{45}
    \foreach \k in {0,1,3,4,...,\lastblockbody}
      \drawthread{thread, gray}{\block}{\k}{\to};

    % draw kernel calls
    \pgfmathsetmacro{\ybot}{(\nrblockbodies + 1) * \block}
    \def\currentx{44}
    \def\currenty{\ybot + 2}
    \draw[kernelcall=brown, pattern=north east lines]
      (   0, \currenty - .2) rectangle ++(64, 1.4);
    \draw[kernelcall=blue!50!black, pattern=north west lines]
      (  40, \currenty - .1) rectangle ++( 8, 1.2);
    \draw[kernelcall=orange, fill]
      (\currentx, \currenty) rectangle ++( 1, 1);
    \node[anchor=base] (interactionanchor) at (\currentx + 0.5, \currenty + 1.0) {};
    \node[anchor=base] (tileanchor)        at (\currentx - 2.5, \currenty + 1.1) {};
    \node[anchor=base] (evalanchor)        at (\currentx - 8.5, \currenty + 1.2) {};

    \node[imglabel] (interactionlabel) at (49.5, \currenty +  4.5) {\textit{interaction}};
    \node[imglabel] (tilelabel)        at (49.5, \currenty +  7.5) {\textit{tile\_computation}};
    \node[imglabel] (evallabel)        at (49.5, \currenty + 10.5) {\textit{evaluation}};

    \draw[kernelarrow, orange]        (interactionanchor.base) to (interactionlabel);
    \draw[kernelarrow, blue!50!black] (tileanchor.base)        to (tilelabel);
    \draw[kernelarrow, brown]         (evalanchor.base)        to (evallabel);

    \drawthread{thread}{\block}{2}{45};

    \def\examplex{21}
    \def\exampley{5}
    \draw[examplecell] (\examplex, \exampley) rectangle ++(1, 1);
    \node[anchor=base] (cellanchor) at (\examplex + .5, \exampley + .5) {};

    \draw (35, -4) node[imglabel, anchor=west] (interaction) {
      \(
       \left\{
       \begin{aligned}
         {\vec U}_{pq} &= α_q\,\K_ε(\x_p - \x_q) \\
         A_{pq}        &= [α_q - α_p]\,η_ε(\x_p - \x_q) \\
       \end{aligned}
       \right.
      \)
    };
    \draw[->, out=-45, in=180] (cellanchor.base) to (interaction);
    \node[imglabel, anchor=east]  (plabel) at (\examplex - 7,  \exampley + .5) {\((x_p, y_p, α_p)\)};
    \node[imglabel, anchor=south] (qlabel) at (\examplex + .5, -1) {\((x_q, y_q, α_q)\)};
    \draw[dotted, very thick] (plabel) -- (cellanchor.base);
    \draw[dotted, very thick] (qlabel) -- (cellanchor.base);

    \foreach \block in {1,2,3,4} {
      \pgfmathsetmacro{\ybot}{(1 + \nrblockbodies) * \block}
      \foreach \thread in {0,...,\lastblockbody} {
        \node[font=\tiny, anchor=east] at (0, .5 + \ybot + \thread) {\thread};
      }
    }

  \end{tikzpicture}
  \caption[Work distribution among threads.]%
    {Work distribution among threads and thread blocks
    for a system of \protect{\(N = 64\)} particles and
    a grid of \protect{\(B = 8\)} thread blocks of \(T = N/B = 8\) threads.
    Each small square represents an interaction between two particles.
    Dashed lines show where threads synchronize
    and fetch a new set of particles into shared memory.
    Waved lines show how threads sequentially compute interactions along a row.
    The rectangles in block 4 show the interactions associated
    to each of the device kernels being called.
  }
  \label{fig:nbody-tiles}
\end{figure}

Figure~\ref{fig:nbody-tiles} shows a schematic representation
of the distribution of work among threads.
All of the~\(N^2\) particle interactions are represented
as cells in the finer grid.
Each thread computes sequentially the interactions
along a row.
In this example, blocks 1 through 4 are already in execution,
while the others have not started yet.

\section{Memory management}
\label{sec:memory-management}

Throughout the simulation,
the current state of the particle system
is stored in an array of particles in global memory,
where each particle is described as a 3-tuple~\(x_p, y_p, α_p\).

On Nvidia GPUs,
global memory reads are always~\emph{coalesced} (accessed in chunks)
and \emph{aligned} to chunk boundaries\fbox{CITE}.
Unaligned accesses imposes a performance penalty,
since more chunks must be fetched
when data spills across boundaries.

Since chunks are aligned to addresses that are
multiples of 32, 64 or 128 (depending of the chunk size),
accesses are more efficient when particles are stored
as values of type~\texttt{float4} instead of~\texttt{float3}
(records of resp.\@ four and three single-precision floating point values)
whose fourth field is kept unused,


\section{Kernel implementation}
\label{sec:kernel-implementation}


