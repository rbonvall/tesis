\chapter{GPU computing}
\label{ch:gpu-computing}

(This chapter's bibliography:
\cite{cudaprog2} \cite{owens08} \cite{jansen07} \cite{lejdfors08}%
)

Graphics processors (GPUs) have evolved in the last decade
from being fixed-function specialized hardware units
for accelerating the graphics pipeline
into programable high-throughput parallel processors,
capable of being used for general-purpose computations.

Programming for the GPUs is markedly different
than for single chip processor though.
Massive parallelism,
hardware constraints,
data access latency and
the impact of low level minutiae on performance
impose a different mindset for implementing an algorithm.
This chapter introduces the concepts
about GPU computing that are relevant to this thesis.

\section{Architecture of a GPU}

Modern programmable GPUs are aggressively pipelined,
multithreaded, manycore processors
specialized in floating point calculations.

The GPU delivers high throughput not by accelerating individual operations
(which are slower than on the CPU),
but by means of executing many threads in parallel.
It achieves this by devoting more transistors to processing cores
rather than data caching and flow control~\cite[\S1]{cudaprog2},
as illustrated by figure~\ref{fig:transistors}.

\begin{figure}
  \centering
  \tikzstyle{trans-name}=[anchor=north west]
  \tikzstyle{alu}=[fill=red!80!black!30]
  \tikzstyle{data}=[pattern color=blue!60, pattern=north east lines]
  \tikzstyle{ctrl}=[pattern color=green!80!black!60, pattern=north west lines]
  \subfigure[CPU] {
    \begin{tikzpicture}[scale=0.8]
      \draw[ctrl] (0, 6)   rectangle +(3, -2);
      \draw[data] (0, 4)   rectangle +(6, -2);
      \draw[data] (0, 1.5) rectangle +(6, -1);
      \node[trans-name] at (0, 6) {Control};
      \node[trans-name] at (0, 4) {Cache};
      \node[trans-name] at (0, 1.5) {DRAM};
      \foreach \x in {3, 4.5}
        \foreach \y in {5, 6} {
          \draw[alu] (\x, \y) rectangle +(1.5, -1);
          \node[trans-name] at (\x, \y) {ALU};
        }
    \end{tikzpicture}
  }
  \subfigure[GPU] {
    \begin{tikzpicture}[scale=0.8]
      \draw[data] (0, 1.5) rectangle +(6, -1);
      \node[trans-name] at (0, 1.5) {DRAM};
      \foreach \x in {0.5, 1, ..., 5.5}
        \foreach \y in {2.5, 3, ..., 6}
          \draw[alu] (\x, \y) rectangle +(0.5, -0.46);
      \foreach \y in {2.5, 3, ..., 6}
        \draw[ctrl] (0, \y) rectangle +(0.5, -0.23);
      \foreach \y in {2.27, 2.77, ..., 6}
        \draw[data] (0, \y) rectangle +(0.5, -0.23);
    \end{tikzpicture}
  }
  \label{fig:transistors}
  \caption[Comparison between CPU and GPU layout]{
    Comparison between the amount of transistors
    devoted to different functions inside a CPU and a GPU.
    %Arithmetic logic units (ALUs) perform data processing.
    (This picture is copied from~\cite[\S1]{cudaprog2}).
  }
\end{figure}

This arrangement is what makes the GPU
well-suited for computations
for which the data is distributed among processing cores
for each of them to execute the same instructions
on a different subset of it.
This is called \emph{data parallelism}.
Data-parallel computations have lower requirements
for sophisticated flow control
and can hide the memory access latency with calculations
instead of caches~\cite[\S1.2]{cudaprog2}.

\section{Parallel computing and the streaming paradigm}



%The programming model of the GPU is classified as single-program multiple-data (SPMD):
%the same instructions are executed in parallel on different data sets.
%
%The two main abstractions of the streaming paradigm are streams and kernels.
%
%A \emph{stream} is a collection of data which can be operated on in parallel.
%
%\emph{Kernels} are special functions which operate on streams.
%Calling a kernel on a stream
%performs an implicit loop over the elements of the stream,
%invoking the body of the kernel for each element.


\section{CUDA}

CUDA is the GPU development environment
introduced by the Nvidia hardware vendor.

CUDA extends the C programming language
with syntatic support for defining and launching kernels
to be executed on a device (in this case, the GPU)
that operates as a coprocessor of the host running the application.
Source files must be compiled with the \texttt{nvcc} compiler,
which separates the code sections meant to be executed on the device
and compiles them into binary objects called \emph{cubins}.

The main abstractions provided by CUDA
are the thread hierarchy and the memory hierarchy.
All programs that make use of the GPU
must be designed and implemented around these concepts.

\subsection{Thread hierarchy}
\label{sub:thread-hierarchy}

A CUDA kernel is a function
that is executed concurrently
by all the threads that are created
at invocation time.

Threads are created in \emph{blocks}
of up to 512 threads.
Threads within a block
execute on the same multiprocessor,
can cooperate by sharing data through shared memory,
and can synchronize.

Each thread in a block
is assigned a unique index
called \verb+threadIdx+.
The size of the block
is called \verb+blockDim+.
Both are variables that are accessible
from all the threads.

Blocks are organized into a \emph{grid}.
Blocks within a grid
are required to execute independently,
so they can be scheduled to idle multiprocessors
in any order at any time.
Each block is assigned a unique index
called \verb+blockIdx+,
that is also accessible from all the threads.

All threads execute the same kernel code,
but have a different value of \verb+threadIdx+ and \verb+blockIdx+,
that can be used to determine on which data they have to work.

For example,
a CUDA kernel to compute the sum \(\vec c = \vec a + \vec b\) of two vectors
can be written like this:
\begin{verbatim}
__global__ void vector_add(float a[], float b[], float c[]) {
    unsigned i = blockIdx.x * blockDim.x + threadIdx.x;
    c[i] = a[i] + b[i];
}
\end{verbatim}
Each thread will fetch one element of each of the input arrays, add them,
and store the sum in the corresponding component of the result.

The \verb+vector_add+ kernel can be executed with any grid configuration,
provided that the arrays have been properly allocated and initialized.
The configuration is specified when executing the kernel
by inserting an expression of the form \texttt{<<<gridDim, blockDim>>>}
between the function name and its argument list.

For example,
two arrays of size 8192 can be added using any of the following configurations:
\begin{verbatim}
vector_add<<<16, 512>>>(a, b, c);
vector_add<<<32, 256>>>(a, b, c);
vector_add<<<64, 128>>>(a, b, c);
\end{verbatim}

\subsection{Memory hierarchy}
\label{sub:memory-hierarchy}

CUDA devices have several kinds of on-chip memory.
The ones relevant to this work are:
\begin{itemize}
  \item per-thread \emph{local memory};
  \item per-block \emph{shared memory}; and
  \item \emph{global memory};
\end{itemize}

Global memory space is persistent across kernel launches
by the same application.
Allocation and data copying is done from the host
by using C functions provided by the CUDA runtime API.

Shared memory is usable from all threads of a block,
and has the same lifetime as the block.
Access to shared memory is much faster than to global memory,
so it is useful as a programmer-managed L1-like cache.
Threads within the block can cooperate among themselves
by sharing data on through it and synchronizing their execution
to coordinate memory accesses.

\section{Application workflow}
\label{sec:app-workflow}

The basic workflow of a simple CUDA application is the following:
\begin{itemize}
  \item allocate arrays on the device global memory;
  \item copy data from the host to the allocated arrays;
  \item launch a kernel to operate on the data;
  \item once the kernel has finished,
    copy the result data back to the host;
  \item free the allocated memory.
\end{itemize}




These abstractions provide support
for both task parallelism and data parallelism.
The strategy this suggests for programming
is decomposing problems into coarse subproblems
that can be solved independently
(this is called \emph{task parallelism}),
and then into finer pieces
than can be solved cooperatively in parallel~\cite[\S1.1]{cudaprog2}.

The CUDA programming model maps to the Tesla GPU architecture.
Nvidia GPU's architecture is built around
a scalable array of multithreaded \emph{streaming multiprocessors}.
When a kernel is invoked,
blocks of the grid are distributed to available multiprocessors,
where threads within a block execute concurrently.
As blocks terminate,
new blocks are assigned to vacated multiprocessors~\cite[\S3]{cudaprog2}.

Each multiprocessor creates, manages and executes threads
on its eight \emph{scalar processors}
in groups of 32 threads called \emph{warps}.
Threads within a warp execute concurrently
but are allowed to diverge.

This architecture,
whose instructions specify
the execution and branching behavior of single threads,
is called \emph{single-instruction multiple-thread} (SIMT) by Nvidia.
SIMT behavior can be ignored for the purposes of program correctness,
but it must be taken into account when developing for peak performance.

