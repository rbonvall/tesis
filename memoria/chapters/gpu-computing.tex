\chapter{GPU computing}
\label{ch:gpu-computing}

(This chapter's bibliography:
\cite{cudaprog2} \cite{owens08} \cite{jansen07} \cite{lejdfors08}%
)

Graphics processors (GPUs) have evolved in the last decade
from being fixed-function specialized hardware units
for accelerating the graphics pipeline
into programable high-throughput parallel processors,
capable of being used for general-purpose computations.

Programming for the GPUs is markedly different
than for single chip processor though.
Massive parallelism,
hardware constraints,
data access latency and
the impact of low level minutiae on performance
impose a different mindset for implementing algorithms.
This chapter presents the concepts
about GPU computing that are relevant to this thesis.

\section{Architecture of a GPU}

Modern programmable GPUs are aggressively pipelined,
multithreaded, manycore processors
specialized in floating point calculations.

The GPU delivers high throughput not by accelerating individual operations
(which are slower than on the CPU),
but by means of executing many threads in parallel.
It achieves this by devoting more transistors to data processing
rather than data caching and flow control~\cite[\S1]{cudaprog2},
as illustrated by figure~\ref{fig:transistors}.

\begin{figure}
  \centering
  \tikzstyle{trans-name}=[anchor=north west]
  \tikzstyle{alu}=[fill=red!80!black!30]
  \tikzstyle{data}=[pattern color=blue!60, pattern=north east lines]
  \tikzstyle{ctrl}=[pattern color=green!80!black!60, pattern=north west lines]
  \subfigure[CPU] {
    \begin{tikzpicture}[scale=0.8]
      \draw[ctrl] (0, 6)   rectangle +(3, -2);
      \draw[data] (0, 4)   rectangle +(6, -2);
      \draw[data] (0, 1.5) rectangle +(6, -1);
      \node[trans-name] at (0, 6) {Control};
      \node[trans-name] at (0, 4) {Cache};
      \node[trans-name] at (0, 1.5) {DRAM};
      \foreach \x in {3, 4.5}
        \foreach \y in {5, 6} {
          \draw[alu] (\x, \y) rectangle +(1.5, -1);
          \node[trans-name] at (\x, \y) {ALU};
        }
    \end{tikzpicture}
  }
  \subfigure[GPU] {
    \begin{tikzpicture}[scale=0.8]
      \draw[data] (0, 1.5) rectangle +(6, -1);
      \node[trans-name] at (0, 1.5) {DRAM};
      \foreach \x in {0.5, 1, ..., 5.5}
        \foreach \y in {2.5, 3, ..., 6}
          \draw[alu] (\x, \y) rectangle +(0.5, -0.46);
      \foreach \y in {2.5, 3, ..., 6}
        \draw[ctrl] (0, \y) rectangle +(0.5, -0.23);
      \foreach \y in {2.27, 2.77, ..., 6}
        \draw[data] (0, \y) rectangle +(0.5, -0.23);
    \end{tikzpicture}
  }
  \label{fig:transistors}
  \caption[Comparison between CPU and GPU layout]{
    Comparison between the amount of transistors
    devoted to different functions inside a CPU and a GPU.
    %Arithmetic logic units (ALUs) perform data processing.
    (This picture is taken from~\cite[\S1]{cudaprog2}).
  }
\end{figure}

This arrangement is what makes the GPU
well-suited for computations
for which the data is distributed among processing cores
for each of them to execute the same instructions
on a different subset of it.
This is called \emph{data parallelism}.
Data-parallel computations have lower requirements
for sophisticated flow control
and can hide the memory access latency with calculations
instead of caches~\cite[\S1.2]{cudaprog2}.




\section{Streaming paradigm}

The programming model of the GPU is classified as single-program multiple-data (SPMD):
several instances of the same program are executed in parallel
on different data.

The two main abstractions of the streaming paradigm are streams and kernels.

A \emph{stream} is a collection of data which can be operated on in parallel.

\emph{Kernels} are special functions which operate on streams.
Calling a kernel on a stream
performs an implicit loop over the elements of the stream,
invoking the body of the kernel for each element.


\section{CUDA}

CUDA is the GPU development environment
introduced by the Nvidia hardware vendor.

CUDA extends the C programming language
with syntatic support for defining kernels
to be executed on the GPU
and a runtime library.
Source files must be compiled with the \texttt{nvcc} compiler,
which separates the code sections meant to execute on the GPU
and compiles them into binary objects called \emph{cubins}.

CUDA provides three key abstractions.
\begin{enumerate}
  \item A hierarchy of threads,
    which comprises, from finer- to coarser-grained:
    \begin{itemize}
        \item single \emph{threads},
          identified by a three-dimensional index;
        \item thread \emph{blocks}
          of up to 512 threads
          that reside on the same processor,
          can cooperate by sharing data
          through local shared memory,
          and can synchronize; and
        \item a \emph{grid} of blocks
          for organizing the blocks
          into which the problem was decomposed.
    \end{itemize}
  \item A hierarchy of shared memory,
    with different addressing and performance characteristics,
    comprising:
    \begin{itemize}
        \item per-thread \emph{registers};
        \item per-thread \emph{local memory};
        \item per-block \emph{shared memory};
        \item per-grid \emph{global memory};
        \item per-grid read-only \emph{constant memory}; and
        \item per-grid \emph{texture memory}.
    \end{itemize}
  \item A lightweight barrier function
    for synchronizing threads within a block.
\end{enumerate}
These abstractions provide support
for both task parallelism and data parallelism.
The strategy this suggests for programming
is decomposing problems into coarse subproblems
that can be solved independently
(this is called \emph{task parallelism}),
and then into finer pieces
than can be solved cooperatively in parallel~\cite[\S1.1]{cudaprog2}.

The CUDA programming model maps to the Tesla GPU architecture.
Nvidia GPU's architecture is built around
a scalable array of multithreaded \emph{streaming multiprocessors}.
When a kernel is invoked,
blocks of the grid are distributed to available multiprocessors,
on each of which threads within a block execute concurrently.
As blocks terminate,
new blocks are launched on vacated multiprocessors~\cite[\S3]{cudaprog2}.

Each multiprocessor creates, manages and executes threads
on its eight \emph{scalar processors}
in groups of 32 threads called \emph{warps}.
Threads within a warp execute concurrently
but are allowed to diverge.

This architecture,
whose instructions specify
the execution and branching behavior of single threads,
is called \emph{single-instruction multiple-thread} (SIMT) by Nvidia.
SIMT behavior can be ignored for the purposes of program correctness,
but it must be taken into account when developing for peak performance.

