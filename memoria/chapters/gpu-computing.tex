\chapter{GPU computing}
\label{ch:gpu-computing}

%(This chapter's bibliography:
%\cite{cudaprog2} \cite{owens08} \cite{jansen07} \cite{lejdfors08}%
%)

Graphics processors (GPUs) have evolved in the last decade
from being fixed-function specialized hardware units
for accelerating the graphics pipeline
into programable high-throughput parallel processors,
capable of being used for general-purpose computations.

Programming for the GPUs is markedly different
than for single chip processor though.
Massive parallelism,
hardware constraints,
data access latency and
the impact of low level minutiae on performance
impose a different mindset for implementing any algorithm.
This chapter introduces the concepts
about GPU computing that are relevant to this thesis.

\section{Architecture of a GPU}

Modern programmable GPUs are aggressively pipelined,
multithreaded, manycore processors
specialized in floating point calculations.

The GPU delivers high throughput not by accelerating individual operations
(which are slower than on the CPU),
but by means of executing many threads in parallel.
It achieves this by devoting more transistors to processing cores
rather than data caching and flow control~\cite[\S1]{cudaprog2},
as illustrated by figure~\ref{fig:transistors}.

\begin{figure}
  \centering
  \tikzstyle{trans-name}=[anchor=north west]
  \tikzstyle{alu}=[fill=red!80!black!30]
  \tikzstyle{data}=[pattern color=blue!60, pattern=north east lines]
  \tikzstyle{ctrl}=[pattern color=green!80!black!60, pattern=north west lines]
  \subfigure[CPU] {
    \begin{tikzpicture}[scale=0.8]
      \draw[ctrl] (0, 6)   rectangle +(3, -2);
      \draw[data] (0, 4)   rectangle +(6, -2);
      \draw[data] (0, 1.5) rectangle +(6, -1);
      \node[trans-name] at (0, 6) {Control};
      \node[trans-name] at (0, 4) {Cache};
      \node[trans-name] at (0, 1.5) {DRAM};
      \foreach \x in {3, 4.5}
        \foreach \y in {5, 6} {
          \draw[alu] (\x, \y) rectangle +(1.5, -1);
          \node[trans-name] at (\x, \y) {ALU};
        }
    \end{tikzpicture}
  }
  \subfigure[GPU] {
    \begin{tikzpicture}[scale=0.8]
      \draw[data] (0, 1.5) rectangle +(6, -1);
      \node[trans-name] at (0, 1.5) {DRAM};
      \foreach \x in {0.5, 1, ..., 5.5}
        \foreach \y in {2.5, 3, ..., 6}
          \draw[alu] (\x, \y) rectangle +(0.5, -0.46);
      \foreach \y in {2.5, 3, ..., 6}
        \draw[ctrl] (0, \y) rectangle +(0.5, -0.23);
      \foreach \y in {2.27, 2.77, ..., 6}
        \draw[data] (0, \y) rectangle +(0.5, -0.23);
    \end{tikzpicture}
  }
  \label{fig:transistors}
  \caption[Comparison between CPU and GPU layout]{
    Comparison between the amount of transistors
    devoted to different functions inside a CPU and a GPU.
    %Arithmetic logic units (ALUs) perform data processing.
    (This picture is copied from~\cite[\S1]{cudaprog2}).
  }
\end{figure}

This arrangement is what makes the GPU
well-suited for computations
for which the data is distributed among processing cores
for each of them to execute the same instructions
on a different subset of the data,
that is distributed among processing cores.
This is called \emph{data parallelism}.
Data-parallel computations have lower requirements
for sophisticated flow control
and can hide the memory access latency with calculations
instead of caches~\cite[\S1.2]{cudaprog2}.

GPUs were originally designed to accelerate graphics applications,
composed of several data-parallel stages arranged as a pipeline.
Current generation GPUs are built around a \emph{unified shader} architecture,
in which all multiprocessors are available for all the stages of the pipeline.
This allows to exploit \emph{task parallelism},
the execution of several stages of an algorithm in parallel:
multiprocessors are allocated for each task as they are needed~\cite[\S{}II]{owens08}.

By having support for both task parallelism and data parallelism,
problems can be decomposed into coarse independent subproblems
and then into finer pieces
than can be solved cooperatively in parallel~\cite[\S1.1]{cudaprog2}.

\section{Streaming paradigm}

The architecture of graphic hardware
is akin to the SIMD class of parallel computers
(typically represented by vector processors),
in that a single instruction controls multiple processing elements~\cite[\S3.1]{cudaprog2}.
However, there are some differences that the SIMD classification does not reflect.

Modern GPUs execute small programs that operate on local registers rather than on memory.
These programs are developed to load and store their data at the thread-level.
Threads are mapped to processor cores, and execute independently
with their own instruction address and register state.

This programming model captures computational locality not present in the SIMD model.
The abstraction that arises is called \emph{stream programming}.
The streaming paradigm models parallel computations through the use of streams and kernels.
A \emph{stream} is a collection of records requiring similar computation,
while \emph{kernels} are functions executed on each element of a stream~\cite[\S2.1]{buck04}.
GPU programming consists mainly in defining kernels
and managing the data to be operated on.

The applications that can benefit from stream processing
are those that exhibit high \emph{arithmetic intensity},
the ratio of computations to memory accesses.
For best performance,
GPU applications have to optimize their memory usage
(data transfers and access patterns)
to achieve maximum memory bandwidth,
while maximizing parallel computations~\cite[\S5.5]{cudaprog2}.

\section{CUDA}

CUDA is the GPU development environment
introduced by the Nvidia hardware vendor.

CUDA extends the C programming language
with syntatic support for defining and launching kernels
to be executed on a device (in this case, the GPU)
that operates as a coprocessor of the host running the application.
Source files must be compiled with the \texttt{nvcc} compiler,
which separates the code sections meant to be executed on the device
and compiles them into binary objects called \emph{cubins}.

The main abstractions provided by CUDA
are the thread hierarchy and the memory hierarchy.
All programs that make use of the GPU
must be designed and implemented around these concepts.

\subsection{Thread hierarchy}
\label{sub:thread-hierarchy}

A CUDA kernel is a function
that is executed concurrently
by all the threads that are created
at invocation time.

Threads are created in \emph{blocks}
of up to 512 threads.
Threads within a block
execute on the same multiprocessor,
can cooperate by sharing data through shared memory,
and can synchronize.

Each thread in a block
is assigned a unique index
called \texttt{threadIdx}.
The size of the block
is called \texttt{blockDim}.
Both are variables that are accessible
by all the threads.

Blocks are organized into a \emph{grid}.
Blocks within a grid
are required to execute independently,
so they can be scheduled to idle multiprocessors
in any order at any time.
Each block is assigned a unique index
called \verb+blockIdx+,
that is also accessible by all the threads.

All threads execute the same kernel code,
but have a different value of \verb+threadIdx+ and \verb+blockIdx+,
that can be used to determine on which data they have to work.

For example,
a CUDA kernel to compute the sum \(\vec c = \vec a + \vec b\) of two vectors
can be written like this:
\begin{verbatim}
__global__ void vector_add(float a[], float b[], float c[]) {
    unsigned i = blockIdx.x * blockDim.x + threadIdx.x;
    c[i] = a[i] + b[i];
}
\end{verbatim}
Each thread will fetch one element of each of the input arrays, add them,
and store the sum in the corresponding component of the result.

The \verb+vector_add+ kernel can be executed with any grid configuration,
provided that the arrays have been properly allocated and initialized.
The configuration is specified when executing the kernel
by inserting an expression of the form \texttt{<<<gridDim, blockDim>>>}
between the function name and its argument list.

For example,
two arrays of size 8192 can be added
by the kernel defined above
by using any of the following configurations:
\begin{verbatim}
vector_add<<<16, 512>>>(a, b, c);
vector_add<<<32, 256>>>(a, b, c);
vector_add<<<64, 128>>>(a, b, c);
\end{verbatim}

\subsection{Memory hierarchy}
\label{sub:memory-hierarchy}

CUDA devices have several kinds of on-chip memory.
The ones relevant to this work are:
\begin{itemize}
  \item per-thread \emph{local memory};
  \item per-block \emph{shared memory}; and
  \item \emph{global memory};
\end{itemize}

Global memory space is persistent across kernel launches
by the same application.
Allocation and data copying is done from the host
by using C functions provided by the CUDA runtime API.

Shared memory is usable from all threads of a block,
and has the same lifetime as the block.
Shared memory is much faster than global memory,
so it is useful as a programmer-managed L1-like cache.
Threads within the block can cooperate among themselves
by sharing data on through it and synchronizing their execution
to coordinate memory accesses.

\subsection{Application workflow}
\label{sub:app-workflow}

The basic workflow of a simple CUDA application is the following:
\begin{itemize}
  \item allocate arrays on the device global memory;
  \item transfer data from the host to the allocated arrays;
  \item launch a kernel;
  \item once the kernel has finished,
    trasfer the result data back to the host;
  \item free the allocated memory.
\end{itemize}

